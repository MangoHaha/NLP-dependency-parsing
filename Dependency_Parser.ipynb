{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dependency_Parser.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MangoHaha/NLP-dependency-parsing/blob/master/Dependency_Parser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "pIuhV5tg0fwp",
        "colab_type": "code",
        "outputId": "22ae4ecb-1b04-40fc-ad11-a6ac0c005132",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        " "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ef/88/8e5f4cfb99a4186b4b7f06aa1294353e0be6b05b25802a82f3d16cb30b79/pyspark-2.4.2.tar.gz (193.9MB)\n",
            "\u001b[K    100% |████████████████████████████████| 193.9MB 80kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.7 (from pyspark)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
            "\u001b[K    100% |████████████████████████████████| 204kB 27.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/dc/0e/02/e9fdf0bf3ad20284175307d4ab31afcf967604f25f3b4f1d96\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.7 pyspark-2.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KTQ3V-Xf1B4E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark import SparkContext\n",
        "sc = SparkContext(\"local\", \"Data Prepare\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rTNI4SD8tmmk",
        "colab_type": "code",
        "outputId": "776c4df7-d397-41f5-a11e-9a746f19efa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hXoklQCeyWUw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vVoUMjpYyAS2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DataSet:\n",
        "  def __init__ (self):\n",
        "    self.dev_data = pd.read_csv('/content/drive/My Drive/NLP/data/dev.data', sep=\" \", header=None)\n",
        "    self.train_data = pd.read_csv('/content/drive/My Drive/NLP/data/train.data', sep=\" \", header=None)\n",
        "    self.vocabs_action = pd.read_csv('/content/drive/My Drive/NLP/data/vocabs.actions', sep=\" \", header=None)\n",
        "    self.vocabs_pos = pd.read_csv('/content/drive/My Drive/NLP/data/vocabs.pos', sep=\" \", header=None)\n",
        "    self.vocabs_label = pd.read_csv('/content/drive/My Drive/NLP/data/vocabs.labels', sep=\" \", header=None)\n",
        "    self.vocabs_word = pd.read_csv('/content/drive/My Drive/NLP/data/vocabs.word', sep=\" \", header=None)\n",
        "    self.word_train = self.prepare_data(train_data, vocabs_word, 0, 20)\n",
        "    self.pos_train = self.prepare_data(train_data, vocabs_pos, 20, 40)\n",
        "    self.label_train = self.prepare_data(train_data, vocabs_label, 40, 52)\n",
        "    self.action_train = self.prepare_data(train_data, vocabs_action, 52, 53)\n",
        "    self.word_dev = self.prepare_data(dev_data, vocabs_word, 0, 20)\n",
        "    self.pos_dev = self.prepare_data(dev_data, vocabs_pos, 20, 40)\n",
        "    self.label_dev = self.prepare_data(dev_data, vocabs_label, 40, 52)\n",
        "    self.action_dev = self.prepare_data(dev_data, vocabs_action, 52, 53)\n",
        "\n",
        "  def prepare_data(self, data, vocabs, start, end):\n",
        "    sub_data = data.iloc[:,start:end]\n",
        "    voc = dict(zip(vocabs.iloc[:,0], vocabs.iloc[:,1]))\n",
        "    new_array = []\n",
        "    #for line in sub_data.head(20).itertuples():\n",
        "    for line in sub_data.itertuples():\n",
        "      cur_array = []\n",
        "      for word in line[1:]:\n",
        "        if word in voc:\n",
        "          cur_array.append(voc.get(word))\n",
        "        else:\n",
        "          cur_array.append(0)\n",
        "      new_array.append(cur_array)\n",
        "    return pd.DataFrame(new_array)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EWmAk093IrN0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class NetProperties:\n",
        "    def __init__(self, word_embed_dim, pos_embed_dim, label_embed_dim, hidden_dim, minibatch_size, epoch):\n",
        "        self.word_embed_dim = word_embed_dim\n",
        "        self.pos_embed_dim = pos_embed_dim\n",
        "        self.label_embed_dim = label_embed_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.minibatch_size = minibatch_size\n",
        "        self.epoch = epoch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mJUalkT8I8Ou",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "outputId": "202be682-ea67-4a74-e8d5-8168a1168a34"
      },
      "cell_type": "code",
      "source": [
        "!pip install dynet\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting dynet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/2c/6b773466b4cca5092b3fac49969011a720e6ca7b01bcfd286ac412a62971/dyNET-2.1-cp27-cp27mu-manylinux1_x86_64.whl (27.9MB)\n",
            "\u001b[K    100% |████████████████████████████████| 27.9MB 962kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python2.7/dist-packages (from dynet) (1.16.3)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python2.7/dist-packages (from dynet) (0.29.7)\n",
            "Installing collected packages: dynet\n",
            "Successfully installed dynet-2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U2q8gqLwIsle",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import dynet as dynet\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "class Network:\n",
        "    def __init__(self, properties, dataset):\n",
        "        self.properties = properties\n",
        "        self.dataset = dataset\n",
        "\n",
        "        # first initialize a computation graph container (or model).\n",
        "        self.model = dynet.Model()\n",
        "\n",
        "        # assign the algorithm for backpropagation updates.\n",
        "        self.updater = dynet.AdamTrainer(self.model)\n",
        "\n",
        "        # create embeddings for words and tag features.\n",
        "        self.word_embedding = self.model.add_lookup_parameters((dataset.vocabs_word.shape[0], properties.word_embed_dim))\n",
        "        self.pos_embedding = self.model.add_lookup_parameters((dataset.vocabs_pos.shape[0], properties.pos_embed_dim))\n",
        "        self.label_embedding = self.model.add_lookup_parameters((dataset.vocabs_label.shape[0], properties.label_embed_dim))\n",
        "\n",
        "\n",
        "        # assign transfer function\n",
        "        self.transfer = dynet.rectify  # can be dynet.logistic or dynet.tanh as well.\n",
        "\n",
        "        # define the input dimension for the embedding layer.\n",
        "        # here we assume to see two words after and before and current word (meaning 5 word embeddings)\n",
        "        # and to see the last two predicted tags (meaning two tag embeddings)\n",
        "        #self.input_dim = 5 * properties.word_embed_dim + 2 * properties.pos_embed_dim\n",
        "        self.input_dim = 20 * (properties.word_embed_dim + properties.pos_embed_dim) + 12*properties.label_embed_dim\n",
        "\n",
        "\n",
        "        # define the hidden layer.\n",
        "        self.hidden_layer1 = self.model.add_parameters((properties.hidden_dim, self.input_dim))\n",
        "\n",
        "        # define the hidden layer bias term and initialize it as constant 0.2.\n",
        "        self.hidden_layer_bias1 = self.model.add_parameters(properties.hidden_dim, init=dynet.ConstInitializer(0.2))\n",
        "        \n",
        "        # define the hidden layer.\n",
        "        self.hidden_layer2 = self.model.add_parameters((properties.hidden_dim, properties.hidden_dim))\n",
        "\n",
        "        # define the hidden layer bias term and initialize it as constant 0.2.\n",
        "        self.hidden_layer_bias2 = self.model.add_parameters(properties.hidden_dim, init=dynet.ConstInitializer(0.2))\n",
        "\n",
        "        # define the output weight.\n",
        "        self.output_layer = self.model.add_parameters((dataset.vocabs_action.shape[0], properties.hidden_dim))\n",
        "\n",
        "        # define the bias vector and initialize it as zero.\n",
        "        self.output_bias = self.model.add_parameters(dataset.vocabs_action.shape[0], init=dynet.ConstInitializer(0))\n",
        "\n",
        "    def build_graph(self,index, word, pos, label):\n",
        "\n",
        "        # extract word embeddings and tag embeddings from features\n",
        "        word_embeds = [self.word_embedding[wid] for wid in word.iloc[index]]\n",
        "        pos_embeds = [self.pos_embedding[wid] for wid in pos.iloc[index]]\n",
        "        label_embeds = [self.label_embedding[wid] for wid in label.iloc[index]]\n",
        "\n",
        "        # concatenating all features (recall that '+' for lists is equivalent to appending two lists)\n",
        "        embedding_layer = dynet.concatenate(word_embeds + pos_embeds + label_embeds)\n",
        "\n",
        "        # calculating the hidden layer\n",
        "        # .expr() converts a parameter to a matrix expression in dynet (its a dynet-specific syntax).\n",
        "        hidden1 = self.transfer(self.hidden_layer1.expr() * embedding_layer + self.hidden_layer_bias1.expr())\n",
        "        hidden2 = self.transfer(self.hidden_layer2.expr() * hidden1 + self.hidden_layer_bias2.expr())\n",
        "\n",
        "        # calculating the output layer\n",
        "        output = self.output_layer.expr() * hidden2 + self.output_bias.expr()\n",
        "\n",
        "        # return the output as a dynet vector (expression)\n",
        "        return output\n",
        "\n",
        "    def train(self):\n",
        "        # matplotlib config\n",
        "        loss_values = []\n",
        "        plt.ion()\n",
        "        ax = plt.gca()\n",
        "        ax.set_xlim([0, 10])\n",
        "        ax.set_ylim([0, 3])\n",
        "        plt.title(\"Loss over time\")\n",
        "        plt.xlabel(\"Minibatch\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "\n",
        "        for i in range(self.properties.epoch):\n",
        "            print 'started epoch', (i+1)\n",
        "            losses = []\n",
        "\n",
        "            step = 0\n",
        "            for index in range(dataset.word_train.shape[0]):\n",
        "                gold_label = dataset.action_train.iloc[index]\n",
        "                result = self.build_graph(index, dataset.word_train, dataset.pos_train, dataset.label_train)\n",
        "\n",
        "                # getting loss with respect to negative log softmax function and the gold label.\n",
        "                loss = dynet.pickneglogsoftmax(result, gold_label)\n",
        "\n",
        "                # appending to the minibatch losses\n",
        "                losses.append(loss)\n",
        "                step += 1\n",
        "                #print(\"G\" + gold_label)\n",
        "                #print(result)\n",
        "                #print(len(losses))\n",
        "                if len(losses) >= self.properties.minibatch_size:\n",
        "                    # now we have enough loss values to get loss for minibatch\n",
        "                    minibatch_loss = dynet.esum(losses) / len(losses)\n",
        "\n",
        "                    # calling dynet to run forward computation for all minibatch items\n",
        "                    minibatch_loss.forward()\n",
        "\n",
        "                    # getting float value of the loss for current minibatch\n",
        "                    minibatch_loss_value = minibatch_loss.value()\n",
        "\n",
        "                    # printing info and plotting\n",
        "                    loss_values.append(minibatch_loss_value)\n",
        "                    if len(loss_values)%10==0:\n",
        "                        ax.set_xlim([0, len(loss_values)+10])\n",
        "                        ax.plot(loss_values)\n",
        "                        plt.draw()\n",
        "                        plt.pause(0.0001)\n",
        "                        progress = round(100 * float(step) / len(train_data), 2)\n",
        "                        print 'current minibatch loss', minibatch_loss_value, 'progress:', progress, '%'\n",
        "\n",
        "                    # calling dynet to run backpropagation\n",
        "                    minibatch_loss.backward()\n",
        "\n",
        "                    # calling dynet to change parameter values with respect to current backpropagation\n",
        "                    self.updater.update()\n",
        "\n",
        "                    # empty the loss vector\n",
        "                    losses = []\n",
        "\n",
        "                    # refresh the memory of dynet\n",
        "                    dynet.renew_cg()\n",
        "\n",
        "            # there are still some minibatch items in the memory but they are smaller than the minibatch size\n",
        "            # so we ask dynet to forget them\n",
        "            dynet.renew_cg()\n",
        "\n",
        "    def decode(self, word_dev, pos_dev, label_dev):\n",
        "        # first putting two start symbols\n",
        "        tags = ['<s>', '<s>']\n",
        "\n",
        "        for i in range(word_dev.shape[0]):\n",
        "\n",
        "            # running forward\n",
        "            output = self.build_graph(i, word_dev, pos_dev, label_dev)\n",
        "\n",
        "            # getting list value of the output\n",
        "            scores = output.npvalue()\n",
        "\n",
        "            # getting best tag\n",
        "            best_tag_id = np.argmax(scores)\n",
        "\n",
        "            # assigning the best tag\n",
        "            tags.append(tag2action.get(best_tag_id))\n",
        "\n",
        "            # refresh dynet memory (computation graph)\n",
        "            dynet.renew_cg()\n",
        "\n",
        "        return tags[2:]\n",
        "\n",
        "    def load(self, filename):\n",
        "        self.model.populate(filename)\n",
        "\n",
        "    def save(self, filename):\n",
        "        self.model.save(filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y1glAYi2UJnD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 631
        },
        "outputId": "e5ffece5-9286-4278-bd1d-304cc7b95f63"
      },
      "cell_type": "code",
      "source": [
        "prop = NetProperties(64, 32, 32, 200, 1000, 7)\n",
        "dataset = DataSet();\n",
        "network = Network(prop, dataset)\n",
        "network.train()\n",
        "network.save(\"/content/drive/My Drive/NLP/model.md5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "started epoch 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEWCAYAAACaBstRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "current minibatch loss 4.87002801895 progress: 6.96 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "current minibatch loss 4.79153060913 progress: 13.91 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "current minibatch loss 4.73096895218 progress: 20.87 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "current minibatch loss 4.67351913452 progress: 27.82 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "current minibatch loss 4.59616374969 progress: 34.78 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "current minibatch loss 4.545566082 progress: 41.74 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "current minibatch loss 4.51279878616 progress: 48.69 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "current minibatch loss 4.47289991379 progress: 55.65 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "current minibatch loss 4.40781021118 progress: 62.61 %\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "current minibatch loss 4.35280895233"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "frlxKXGg1BSH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "fe620402-0654-4cb5-d955-f2f93b564458"
      },
      "cell_type": "code",
      "source": [
        "network.load(\"/content/drive/My Drive/NLP/model.md5\")\n",
        "tag2action = dict(zip(vocabs_action.iloc[:,1], vocabs_action.iloc[:,0]))\n",
        "network.decode(dataset.word_dev, dataset.pos_dev, dataset.label_dev)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['RIGHT-ARC:det',\n",
              " 'RIGHT-ARC:det',\n",
              " 'RIGHT-ARC:det',\n",
              " 'RIGHT-ARC:det',\n",
              " 'RIGHT-ARC:det',\n",
              " 'RIGHT-ARC:det',\n",
              " 'RIGHT-ARC:det',\n",
              " 'RIGHT-ARC:det',\n",
              " 'RIGHT-ARC:det',\n",
              " 'RIGHT-ARC:det',\n",
              " 'RIGHT-ARC:det',\n",
              " 'RIGHT-ARC:det',\n",
              " 'RIGHT-ARC:det',\n",
              " 'RIGHT-ARC:det',\n",
              " 'RIGHT-ARC:det',\n",
              " 'RIGHT-ARC:det',\n",
              " 'RIGHT-ARC:det',\n",
              " 'RIGHT-ARC:appos',\n",
              " 'RIGHT-ARC:det',\n",
              " 'RIGHT-ARC:det']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    }
  ]
}